---
layout: default
title: Home
---
                <div id="corps_central"> 
            
                    <div id="corps_central"> 
                        <h1 style="text-align: center;"><u><b>Evaluation</b></u></h1>
                        <!-- <h1>How Well Does Our System Perform?</h1><p></p> -->


                        <p>
                        We ran a number of tests in simulation to demonstrate the effectiveness of Dr.Phil's components, as well as to make decisions on the best method to use based on empirical evidence.
                        </p>
                        <h2>SLAM navigation testing</h2>
                        <p>We used three different worlds to evaluate the effectiveness of the SLAM mapping stack and see if it sufficiently maps out a normal working environment.</p>
                        <img class="images" style="width: 70%" src="images/evaluation/diffrooms.png">

                        <p>We ran the mapping behaviour of Dr.Phil in each world, and compared the results to a ground truth occupancy map we generated for each map. A confusion matrix was generated like for a binary classifier where positives signified occupied cells and negatives the free cells.</p>

                        <img class="images" style="width: 70%" src="images/evaluation/mappingresults.png">
                        <p>As seen above, Dr.Phil performed best in Arena, and was good in House as well. The issue with C-room was that it's spacious and feature-less, so SLAM struggled to capture the true area. However the shape was still mostly correct. As most offices and hospital rooms are feature-rich, we believe our mapping will perform well in our main target markets.</p>

                        <p>The probability of a collision of the robot with a blind pre-programmed agent was estimated at ~1/3 in another experiment where maps generated were given to the robot while it was tasked to follow a pre-recorded trajectory (3 times), twice, once with un-mapped static and dynamic obstacles (moving back and forth through the robot's path at 0.5 m/s). Note that only collisions in which the robot did not attempt to avoid the obstacle were counted. The true and localized path was also measured, no correlation between localization error and the presence of obstacles was found. The absolute localization error per measurement was on average 0.03m in arena and house worlds and 0.3m in c-room. This is to be expected since the c-room map did not preserve the area or length of the room, otherwise the robot localized very accurately using reasonable maps.</p>
                        <h2>Door handle recognition</h2>
                        <p>The door handle recognition algorithm had to be fast enough to process the video feed from the camera, but also detect all of the door handles that are seen (higher recall). After generating our final door handle dataset, we decided to train some different models to test their performance on classification.</p>
                        <div style="display: block; margin:2px; width: 100%; overflow: hidden;">
                          <div style="width: 50%; float: left; display:block">
                            <img class="images" src="images/evaluation/recognitioncomp.png">
                          </div>
                          <div style="width: 37%; float: right; display:block;">
                          <figure>
                            <img class="images" src="images/evaluation/Performance-of-tiny-YOLOv3-and-YOLOv3-on-the-COCO-Dataset-Redmon-2020.ppm(1).png">
                            <figcaption>YOLO and its other versions' performances compared to other models. Trained on Microsoft COCO dataset - Image taken from the <a href="https://pjreddie.com/darknet/yolo/">authors of YOLO's work</a></figcaption>
                          </figure>
                          </div>
                        </div>
              
                        <h3 style="font-size:large"><b>Comparing the different models</b></h3>
                        <p>Our initial plan was to use Logistic Regression to classify whether an image frame contained a door. If the classifier predicts the possibility of a door in an image, we would process it to detect where that object is. This was done with the intention to maximise efficiency; however, while the classification evaluation results reveal more accuracy (higher mAP) and higher recall (most of the relevant results are returned to us) than YOLOv3, they performed poorly upon classifying doors from afar in implementation testing. This may be attributed to being overfit to the closer samples in our dataset, unlike YOLOv3 which combats it through techniques such as batch normalisation and weight decay. Thus we chose to opt for one of the YOLO models.</p>
                        <p>Hence, since efficiency is important to us as the Raspberry Pi isn't very powerful and the recognition has to be real time, we decided to trade off accuracy for speed. In fact, from the research papers, we also identified that YOLOv3 and its tinier versions are still relatively fast compared to CNNs due to it's implementation.</p>
                        <h3 style="font-size:large"><b>YOLO Performance?</b></h3>
                        <p>When we evaluated the performance, we noticed a very low number on recall for handles, since we thought dividing the recall instead of averaging them would tell us more about the behaviour of the model. The lower f1 score for the tiny version of YOLO versus the YOLO model further demonstrates the imbalance between precision and recall. Upon reflection, these numbers quantify what we were  seeing in our implementation testing, in that it had difficulty in detecing handles from a distance. A possible explanation could be that the input size 416x416 is still very little  for  Tiny  YOLO  to  infer accurately and may perform poorly on small sized objects.  Snippets at a particular distance using different versions of the model is shown in the figure below.</p>
                        <figure>
                          <img class="images" style="width: 70%;" src="images/evaluation/yolocomparison.png">
                          <figcaption>Input Image size passed to the models: a) 416x416 b) 416x416 c)192x192; On OpenCV-DNN (CPU only), 16 GB RAM - Macbook Pro-2015. The numbers next to the class names are confidence scores.</figcaption>
                        </figure>
                          <p>Ultimately, the low recall is not an issue because if we can essentially detect the  door with a confidence value of 0.30 and higher, then we could get closer to the door, and the model will have more pixel information to determine where the handle is in the door. </p>
                          <ul>Other alternatives that the model could proceed in the future: 
                          <li><i>Choosing YOLOv3 with lower input size</i>: We could look into ways of accelerating the performance by using a <a href="https://coral.ai/docs/accelerator/get-started/">USB accelerator</a>. The main issue with the YOLOv3 model was that the FPS (frames per second) was very low, ~0.5 FPS at the same 416x416 resolution, depsite returning much better results. So, getting a tool that can help accelerate the ML models would help us get the power of both accuracy and increasing speed. This will involve converting our current models to a Tensorflow or the appropriate type.</li>
                          <li><i>Further improving detection results on handles</i>: This can be done by training it with more examples of what handles look like from afar and reduce more instances of overfitting in the dataset.</li>
                          </ul>
                          

                            <h2>Spray coverage test</h2>
                        <p>The disinfecting behaviour has to perform excellent, as a improperly disinfected door would still potentially be dangerous. Gazebo doesn't have a "spray" plugin where we can measure the coverage easily. Therefore, we had to use a depth camera on the arm where the nozzle is. The depth camera generates a cloud of points when it hits an object. The camera specification is modified so the view has the same shape as the nozzle spray. We then combine all of the pixels generated over the course of disinfection and compare it to the base handle to get an estimate of the total coverage. We used this estimate to get an idea of the quality of our arm/disinfectant planning.</p>
                        <img class="images" style="width: 70%;" src="images/evaluation/coverage-demo-4.png">
                        <p>We found that Dr. Phil was able to disinfect approximately 85% of the surface of interest in our tests, when only accounting for direct spraying area (no drippage), the true coverage would likely be above this value.</p>


                        <h2>User studies</h2>
                        <p>We performed a quick user study on the app. The tests were primarily to make sure that all of the app's functionality was clear, even to first-time users unfamiliar with the system. We also wanted to make sure the app was accessible and visually coherent. As we've only developed an iOS app, half of the testers would be likely be unable to use the app. With this in mind, we created a google form which showcased the main app screens.</p>

                        <p>We gathered our participants from another SDP group, and received 7 responses in total. Although this is a relatively low number, as stated in the accessibility workshop only 5 participants is enough to capture the super-majority of obvious problems.</p>

                        <p>We asked the particpants what they would do to perform an emergency halt from relevant app screen, as well as reading scheduling information from the app. We also asked them to rate the design/consistency of the UI out of 5, and open-ended questions about parts of the UI that they felt were not clear.</p>

                        <table class="table table-striped table-light">
                            <thead class="table-dark">
                              <tr>
                                <th scope="col">Question</th>
                                <th scope="col">User 1</th>
                                <th scope="col">User 2</th>
                                <th scope="col">User 3</th>
                                <th scope="col">User 4</th>
                                <th scope="col">User 5</th>
                                <th scope="col">User 6</th>
                                <th scope="col">User 7</th>
                                <th scope="col">%/mean</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th scope="row">How often does the Dr.Phil clean on weekends?</th>
                                <td>Wrong</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>6/7</td>
                              </tr>
                              <tr>
                                <th scope="row">What would you press to stop Dr.Phil from this page?</th>
                                <td>Wrong</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Correct</td>
                                <td>Wrong</td>
                                <td>5/7</td>
                              </tr>
                              <tr>
                                <th scope="row">How visually consistent is the app?</th>
                                    <td>5</td>
                                    <td>5</td>
                                    <td>5</td>
                                    <td>5</td>
                                    <td>4</td>
                                    <td>5</td>
                                    <td>5</td>
                                    <td>4.8</td>>
                              </tr>
                              <tr>
                                <th scope="row">How visually appealing is the app?</th>
                                <td>3</td>
                                <td>4</td>
                                <td>3</td>
                                <td>5</td>
                                <td>3</td>
                                <td>5</td>
                                <td>5</td>
                                <td>3.4</td>
                              </tr>

                            </tbody>
                        </table>

                        <p>From the user study, we found some users had issues with submitting the stop request as the button is not in an obvious location. We also found that the word select appeared too much like a pressable option. The app was also not particularly visually appealing, but this is something easily improveable before launch. </p>


                    </div>	
                </div>

