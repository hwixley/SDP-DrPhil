---
layout: default
title: Home
---
                
                <div id="corps_central"> 
            
                    <h1 style="text-align: center;"><b><u>How does our System Work?</u></b></h1>

                    <figure>
                        <img class="images" src="images/components.png">
                        <figcaption style="text-align: center;"><i>Main Dr.Phil components</i></figcaption>
                    </figure>
                    

                    <p>Dr. Phil consists of 5 interconnected sections:
                        <ul class="beginInd">
                            <li><a href="#navigation">A navigation stack </a> which maps the environment and moves the robot to it's objectives.</li>
                            <li><a href="#disinfection">A disinfecting stack </a> that pumps disinfectant to a nozzle located at the top of the arm.</li>
                            <li><a href="#armcontrol">An arm movement stack </a>that can grip doors as well as move for maximum spray coverage.</li>
                            <li><a href="#app">An iOS app </a> that allows users to controls the robot at a high level.</li>
                            <li><a href="#vision">A vision stack</a> which can detect door handles.</li>
                        </ul>

                        The whole system is implemented in ROS, where each thread is implemented as a node. This is all ran on a Raspberry Pi, which also handles communications between the various sensors and the app. To control the behaviour of the robot and manage the individual stacks together, we implemented a <a href="#behaviour">behaviour tree </a> behaviour tree as a single node which is ran when a cleaning routine is started.

                    </p>
                    


                    <p>Here is a sequence diagram which shows an example use case, this process would be planned and controlled by the behaviour tree node.</p>
                    <img class="images" src="images/sequence_diagram.png">
                    <h2 id="behaviour">Behaviour tree</h2>
                    <p>To tie all the robot tasks, behaviour trees were employed. This is due to the need for modular behaviour composition, and reactivity,
                      modularity, as well as ease of maintenance and readability. Alternatives considered, were finite state machines and their variations - but those were dismissed on account of the vast number of transitions required and the difficulty in maintaining,testing and growing complex behaviours as we go.
                        <img class="images" style="max-width: 70%;" src="images/create_explore_frontier_and_save_map.png">
                        <figcaption>Behaviour tree for exploring a room and saving the map</figcaption>
                        <p>The general structure of the project was designed to promote re-use of sub-trees of various sizes and test-ability of individual components.

                            We used the py_trees and py_trees_ros ROS packages as the basic behaviour tree frameworks, both of which provide excellent documentation, backward compatibility and flexibility for extension.</p>

                            <p>So, behaviour trees are a tree shaped model of execution flow. Each leaf in the tree describes an action to be per-formed, which when "ticked" returns one state out of "FAILURE","RUNNING", "SUCCESS", and then execution flow is modified appropriately by parent nodes.	
		
	                            Composite nodes can have multiple children and allow for the combination of multiple actions into a plan. An example of such node is the "Sequence" node which ticks all of its children in order. It does that until one returns with "FAILURE", at which point it prop-agates this state up the tree, or until all of its children return with"SUCCESS" at which point the Sequence propagates "SUCCESS"up the tree instead. Every time a child node is "RUNNING", so is the parent. For more information on behaviour trees please visit <a href="https://www.gamasutra.com/blogs/ChrisSimpson/20140717/221339/Behavior_trees_for_AI_How_they_work.php">Chris Simpson's guide</a></p>	
	                    

                    <h2 id="navigation">Navigation</h2>

                    <i><h4>Exploration</h4></i>	
                    <p>The navigation system uses SLAM for mapping the robots environment, specifically, the gmapping package since it had the best performance during testing and it is relatively lightweight and customisable. The explore_lite and frontier_exploration package allows the robot to roam a space and create an occupancy map without human input.</p>	


                    <i><h4>Door Handle Localisation</h4></i>	
                    <p>The door handle localisation algorithm uses geometry, the correspondence between lidar and camera rays to find out the 3D coordinates of the handle features within the image. We select the point of interest on our image, generate a 3D ray from the camera corresponding to all points mapping to that pixel, then perform intersection checks between this ray and the line segments formed by each consecutive lidar hit point. 	
                    The algorithm assumes the door is a plane perpendicular to the floor, and that the handle feature is placed roughly 4cm away from the door. Once the algorithm computes the coordinates, the robot can then localise itself infront of the door so it is ready to disinfect the handle.</p>	


                    <h2 id="disinfection">Disinfection</h2>
                        <figure>
                            <img class="images" src="images/fixingslabelled(1).jpg">
                            <figcaption style="text-align: center;"><i>Spray components</i></figcaption>
                        </figure>

                        
                        <br>
                        <p>The disinfection stack is primarily hardware based. We have a pump which pumps water from the base of the turtlebot  to the nozzle located at the top of the arm. This is necessary because the liquid storage tank is heavy and would unbalance the arm if installed on it. As the tank is directly on top of the turtlebot, any spillage would be disastrous. Therefore the tank has a screw cap so it won't spill during normal operation whilst still being easy to remove and refill. </p>

                        <i><h4>The Nozzle</h4></i>
                        <img class="images" style="width: 70%" src="images/nozzleCAD.png">
                        <figcaption><i>Nozzle in CAD</i></figcaption>
                        <p>To spray the disinfectant onto contact points, we ultimately decided to use a 60 degree full-cone nozzle, which evenly distributes the disinfectant inside it's spray cone. This is important to ensure the spray will cover contact points efficently and accurately as seen in the <a href="./evaluation.html">evaluation</a> section. A further benefit to this is that the spray simulation we created in gazebo has similar properties to the full cone (namely a even and full spread) so we can be sure that it's a good representation of the real functionality. </p>

                        <i><h4>Control</h4></i>
                        <p>Dr. Phil controls the pump via the 12V openCR board on the turtlebot, which is itself connected to a motor board. This is necessary because the pump has to be relatively powerful to be able to lift the water up to door handle height, and the Raspberry Pi's GPIO can output only 3.3V. We also have a LED strip which was used to simulate when spraying was happening, which is useful for informing nearby people about the status of Dr.Phil.</p>

                        <p>As seen in the image, we also had to create fixings to keep everything still, as the robot might accidentally dislodge components when moving.</p>
                    <h2 id="armcontrol">Arm control</h2>
                    <i><h4>Motion Planning</h4></i>	
	                      <p>In order to move Dr. Phil's spray arm to disinfect doors handles, we use the MoveIt motion planning library which figures out the complex constraints and secondary goals of the arm.</p>	
		
	                      <i><h4>Arm Control</h4></i>	
		
	                      <i><h4>Sanitisation Path Planning</h4></i>	
	                      <p>The sanitisation path planning script uses the coordinates of the center of the handle and the vector normal to the door as inputs. The center of handle is found through object detection and getting the center of the image of the bounding box generated. These inputs are then used to calculate output coordinates on 3 different x-y axes which ensures each angle of the handle is covered, and are all 10cm away from the handle to maximise spray efficiency. Our spray diameter is currently larger than the width of our custom handle so only a single spray on the given x-z axes is required. In the future, we can adapt the distance of spraying to account for wider handles. After this, the z coordinates for these set of x,y points are calculated, this is done by considering the height of the door handle, and replicating these x,y points at multiple heights, described by the z coordinate. These set of 3d coordinates ensure the door handle is fully disinfectant and the whole handle has been covered.</p>    


                    <h2 id="vision">Vision</h2>
                    <p>For our image recognition system we decided to use a version of YOLOv3, a neural network model used for real-time object detection (inc link), due to its fast and accurate output. More specifically, we decided to use YOLOv3-Tiny-PRN (PRN stands for Partial Residual Network) since it is faster in computation speed compared to other versions.	
		
                      To train our model, we made a custom dataset of doors and door handles from footage of the robot exploring various Gazebo worlds. This was done since online datasets were not suitable for our mini door model we created in simulation. We extracted the frames at 10 fps and had to manually label the bounding boxes for the doors and handles. We trained the YOLO models using the darknet repository maintained by AlexeyAB (inc link from demo 2).</p>	
  
                    <p>To carry out the door and door handle recognition algortihm, we use a Raspberry Pi camera is used along with the recognition model, here is a video which showcases the door recognition in action:</p>
                      <video class="images" controls>
                        <source src="images/doorvision.mp4">
                      </video>
                      <figcaption>Door & handle detection</figcaption>
                    <h2 id="app">App</h2>
                        <p>The iOS app is the preferred method of communicating with Dr.Phil. We decided to develop for iOS first because it has a market majority in the UK. We decided to use firebase, a cloud based noSQL storage solution, to bridge the connection between the two. We chose Firebase primarily as it's easily accessible from any platform, and our immediate goals involve developing an android/web app so Dr. Phil can be controlled on anything. Firebase was integrated into the app via cocoapods, and Dr.Phil via python libraries. The app was built using Swift.</p>

                      <video class="images" controls>
                        <source src="images/app_user_example.mp4">
                          
                      </video>
                      <figcaption>Updating firebase from app</figcaption>
                        <p>For the user to actually communicate, they would first securely login to the app, establishing a connection with firebase.</p>
                        <div class="container-fluid">
                            <div class="row">
                              <div class="col-md-6">
                                
                                <figure>
                                    <img class="images" src="images/appregister.png">    
                                    <figcaption>Register account</figcaption>
                                </figure>
                              </div>
                              <div class="col-md-6">
                                <figure>
                                    <img class="images" src="images/approbotconn.png">
                                    <figcaption>Connect to robot</figcaption>
                                </figure>
                              </div>
                              
                            </div>
                          </div>
                        
                        <p>Changing settings in the app would then push those settings to firebase. A local server regularly reads the firebase settings and sends those to Dr.Phil, which then updates itself accordingly. Firebase would easily allow an user to connect to to more than one Dr.Phil at a time, and minimises setup necessary to pair up the app and the bot.</p>
                        <div class="container-fluid">
                            <div class="row">
                              <div class="col-md-6">
                                
                                <figure>
                                    <img class="images" src="images/setschedule.png">    
                                    <figcaption>Sets schedule for Dr.Phil</figcaption>
                                </figure>
                              </div>
                              <div class="col-md-6">
                                <figure>
                                    <img class="images" src="images/stats.png">
                                    <figcaption>Dr.Phil statistics</figcaption>
                                </figure>
                              </div>
                              
                            </div>
                          </div>
                        <p> Dr.Phil also pushes statistics to firebase via it's python library, which is then viewable on the app. </p>


                </div>	
            </div>

