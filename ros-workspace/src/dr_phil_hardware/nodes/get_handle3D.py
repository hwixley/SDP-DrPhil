#!/usr/bin/env python
import rospy
from sensor_msgs.msg import Image as ImageMSG
from cv_bridge import CvBridge, CvBridgeError
import sys
import cv2
import numpy as np
from sensor_msgs.msg import CameraInfo, LaserScan
from std_msgs.msg import Float64MultiArray
import tf
import time
from geometry_msgs.msg import PointStamped,Point,PoseArray,Pose,PoseStamped
from visualization_msgs.msg import Marker,MarkerArray
from tf2_msgs.msg import TFMessage

#Library to process the image and try to find the bounding box of a handle
from dr_phil_hardware.ML.models import yolov3, DEFAULT_WEIGHTS, DEFAULT_CONFIGURATION, DEFAULT_OBJ_NAMES, visualise_results, load_network_and_classes
from dr_phil_hardware.ML.models import YOLOBoundingBox

from dr_phil_hardware.vision.localisation import *
from dr_phil_hardware.vision.camera import Camera
from dr_phil_hardware.vision.lidar import Lidar
from dr_phil_hardware.vision.ray import Ray
from dr_phil_hardware.vision.localisation import localize_pixel
from dr_phil_hardware.vision.utils import invert_homog_mat, quat_from_yaw

from dr_phil_hardware.vision.vision_handle_axis_algorithm import define_handle_features_heursitic as get_center
from dr_phil_hardware.vision.vision_handle_axis_algorithm import Handle

from dr_phil_hardware.disinfection.spray_path import get_spray_path_poses

import traceback

"""
#The purpose of this node: 
(1) Take image and Run an Object Detection Model that helps us find handles and doors in an image
(2) Retrieve the important feature(s)/point(s) (in this case we are only interseted in the center of a pull door handle)
(3) Take the point(s) and transform it from coordinates in a 2D image to 3D coordinates with respect to the robot frame
(4) Publish the vector normal to a vertical surface (generated by the function localise_pixel) and the 3D center point of the handle
"""
class Handle3DTransformation:
    def __init__(self, weights, cfg):
        rospy.init_node('handle2D_to_3D',anonymous=True)
        self.weights = weights
        self.cfg = cfg


        self.robot_frame = "base_link"
        self.lidar_frame = "base_scan"
        self.map_frame = "map"

        # initialize the bridge between openCV and ROS
        self.bridge = CvBridge()


        # Stores the number of frames recieved and to be processed so far
        self.frame_id = 0
        #Store starting time to keep track of elapsed time
        self.starting_time = time.time()
        #Load the network and classes earlier - Done in order to improve efficiency of the node to process images quicker instead of taking time to load
        self.net, self.out, self.classes = load_network_and_classes(self.weights, self.cfg)
        
        self.tranform_listener = tf.TransformListener()
        self.rob2map = None 
        self.camera = None
        self.image_stamp = None
        self.scan = None 

        #Contains YOLO results in an object of type BoundingBox 
        self.handle_box = None
        self.door_box = None


        self.camera_info_sub = rospy.Subscriber("/camera_info",CameraInfo,callback=self.camera_info_callback)
        self.image_sub = rospy.Subscriber("image", ImageMSG, self.yolo_callback)
        self.scan_sub = rospy.Subscriber("/scan_filtered",LaserScan,callback=self.scan_callback)

        #Initialise image publisher to send the calculated 3D world coordinates of handles from a camera image as well as normal to a vertical surface
        self.handle_pose_pub = rospy.Publisher("/handle_feature/pose",PoseStamped,queue_size=10)
        self.door_pose_pub = rospy.Publisher("/door/pose",PoseStamped,queue_size=10)
        self.spray_path_pub = rospy.Publisher("/spray_path/target_points",PoseArray,queue_size=10)
        self.markers_pub = rospy.Publisher("/handle_feature/camera_points",MarkerArray,queue_size=10)

                
    def yolo_callback(self,rgb_msg : ImageMSG):

        # we need to store the time of the image, to know where the robot was 
        # at that time
        self.image_stamp = rgb_msg.header.stamp

        #Reset
        self.handle_box = None
        self.door_box = None

        try:
            self.rgb_image = self.bridge.imgmsg_to_cv2(rgb_msg, desired_encoding="bgr8")
            #Increment the number of frames to be processed
            self.frame_id += 1
        except CvBridgeError as e:
            print(e)

        #Pass the image and the appropriate arguments to get results
        results = yolov3(self.rgb_image,weights=self.weights,cfg=self.cfg, network=self.net, output_layers=self.out, class_names=self.classes)
        
        #See the YOLO results by calling the visualise results function
        visualise_results(self.rgb_image, results, self.starting_time, self.frame_id)

        #Filter the results to have handles only - for now
        for box in results:
            #Get the first handle result
            if box.label=="handle":
                self.handle_box = box
            if box.label =="door":
                self.door_box = box
            

    def scan_callback(self,scan : LaserScan):
        self.scan = scan 


    def lookup_transforms(self):
        """
            Looks up required transforms at the appropriate times, will block for at most 0.1 seconds
        """

        # find the map transform at the time we received the image
        try:
            transformer_ros = tf.TransformerROS()
            self.tranform_listener.waitForTransform(self.map_frame,self.robot_frame,self.image_stamp,rospy.Duration(0.1))
            (trans,rot) = self.tranform_listener.lookupTransform(self.map_frame,self.robot_frame,self.image_stamp)
            self.rob2map = transformer_ros.fromTranslationRotation(trans,rot)

        except Exception as E:
            rospy.logerr_throttle(2,E)
            return
        

    def camera_info_callback(self,camera_info : CameraInfo):
        
        self.camera = Camera(camera_info)    

        transform_listener = tf.TransformListener()
        transformer_ros = tf.TransformerROS()

        # this will block
        rospy.wait_for_message("/tf",TFMessage)


        # wait for camera transform to become available
        transform_listener.waitForTransform(
            self.camera.get_frame_id(),
            self.robot_frame,
            rospy.Time.now(),
            timeout=rospy.Duration(2))

        # get transform
        (trans,rot) = transform_listener.lookupTransform(
            self.camera.get_frame_id(),
            self.robot_frame,
            rospy.Time.now())

        try:
            self.rob2cam = transformer_ros.fromTranslationRotation(trans,rot)
            self.camera.setup_transform(self.rob2cam)
        except (tf.ConnectivityException,tf.LookupException,tf.ExtrapolationException):
            rospy.logerr("Could not find camera transform!")


        self.lidar = Lidar()
        # wait for lidar transform to become available

        transform_listener.waitForTransform(
            self.lidar_frame,
            self.robot_frame,
            rospy.Time.now(),
            timeout=rospy.Duration(2))

        # get lidar transform
        (trans,rot) = transform_listener.lookupTransform(
            self.lidar_frame,
            self.robot_frame,
            rospy.Time.now())

        try:
            self.rob2lid = transformer_ros.fromTranslationRotation(trans,rot)
            self.lidar.setup_transform(self.rob2lid)
        except (tf.ConnectivityException,tf.LookupException,tf.ExtrapolationException):
            rospy.logerr("Could not find camera transform!")

        # chain transforms to get cam2lid
        self.cam2lid = self.rob2lid @ invert_homog_mat(self.rob2cam)

        rospy.loginfo("Received camera info")

        # get rid of subscriber
        self.camera_info_sub.unregister()



    def create_point_from_vec(self,vec,id):
        pnt = Marker()

        pnt.header.frame_id = self.robot_frame
        pnt.header.stamp = self.image_stamp

        pnt.ns = "handle_features"
        pnt.id = id
        pnt.type = 2 # sphere
        pnt.pose.orientation.w = 1
        pnt.color.a = 1
        pnt.color.b = 1
        pnt.action = 0 # add/modify
        pnt.scale.x = 0.05
        pnt.scale.y = 0.05
        pnt.scale.z = 0.05
        pnt.lifetime = rospy.Duration(10)
        pnt.frame_locked = True
        pnt.pose.position.x = vec[0,0]
        pnt.pose.position.y = vec[1,0]
        pnt.pose.position.z = vec[2,0]

        return pnt 

    def create_arrow_from_ray(self,ray,id):
        arw = Marker()

        arw.header.frame_id = self.robot_frame
        arw.header.stamp = self.image_stamp

        arw.ns = "handle_features"
        arw.id = id
        arw.type = 0 # arrow

        p1 = Point()
        orig = ray.origin
        p1.x,p1.y,p1.z = (orig[0],orig[1],orig[2])
        
        p2 = Point()
        p2.x,p2.y,p2.z = (ray.dir[0]+ orig[0],ray.dir[1]+ orig[1],ray.dir[2]+ orig[2]) 

        arw.points = [p1,p2]
        arw.pose.orientation.w = 1
        arw.color.a = 1
        arw.color.b = 1
        arw.action = 0 # add/modify
        arw.scale.x = 0.01
        arw.scale.y = 0.02
        arw.scale.z = 0.05
        arw.lifetime = rospy.Duration(10)
        arw.frame_locked = True

        return arw

    def visualise(self,point_handle,point3d,normal):
        """Visualises the localized points given the point of interest in image pixels and the 3d points of the handle and normal in map frame

        Args:
            point_handle ([type]): [description]
            point3d ([type]): handle point in 3d self.map_frame frame
            normal ([type]): normal in 3d in self.map_frame frame
        """
        camera_ray = self.camera.get_ray_through_image(point_handle)

        markers = MarkerArray()

        camera_mrkr = self.create_arrow_from_ray(camera_ray,0)
        camera_mrkr.header.frame_id = self.camera.get_frame_id()
        
        if point3d is not None:
            point3dmrkr = self.create_point_from_vec(point3d,2) 
            point3dmrkr.header.frame_id = self.map_frame
            normal3dmrkr = self.create_arrow_from_ray(normal,3)
            normal3dmrkr.header.frame_id = self.map_frame
            markers.markers = [camera_mrkr,point3dmrkr,normal3dmrkr]
        else:
            markers.markers = [camera_mrkr]

        self.markers_pub.publish(markers)




    def publish(self,point3d, normal : Ray, publisher:rospy.Publisher):
        """publishes the given point3d and normal, assuming they are given in the self.map_frame frame

        Args:
            point3d ([type]): point in 3d self.map_frame frame
            normal (Ray): normal in 3d in self.map_frame frame
            publisher (rospy.Publisher): provide the topic to publish to of type rospy.Publisher
        """
        if point3d is not None and normal is not None: # normal and point3d are either None together or actual numbers

            # publish pose (facing inwards)
            ps = PoseStamped()
            ps.header.frame_id = self.map_frame
            ps.header.stamp = self.image_stamp
            p = Pose()
            ps.pose = p

            p.position.x,p.position.y,p.position.z = point3d[0:3]
            p.orientation.x, p.orientation.y, p.orientation.z, p.orientation.w = quat_from_yaw(
                -angle_between_pi(
                    normal.get_vec(),
                    np.array([[1],[0],[0]]),
                    plane_normal=np.array([[0],[0],[1]])))

            publisher.publish(ps)
    
    def publish_spray_poses(self,point3d, normal):
        """ publishes the spray origin poses around the handle once it calculates them given the point and the normal in map frame
            Args:
                point3d ([type]): handle point in 3d self.map_frame frame
                normal (Ray): normal in 3d in self.map_frame frame
        """
        if point3d is not None and normal is not None:
            spray_origin_poses = get_spray_path_poses(point3d,normal.dir,self.map_frame)

            # publish target poses
            self.spray_path_pub.publish(spray_origin_poses)
    
        
        

    def spin(self):
        """ perform periodic function of the node, to be called regularly
        """
        self.lookup_transforms()


        if self.camera and self.scan and self.rob2map is not None:
            #Handle
            if self.handle_box is not None:
                (point2d,point3d,normal) = self.convert_to_3D(self.handle_box, is_handle=True)
                try:
                    #Trasform to map frame
                    point3d_map = self.rob2map @ np.append(point3d,[[1]],axis=0)
                    normal_map = normal.get_transformed(self.rob2map)
                    self.publish(point3d_map,normal_map,self.handle_pose_pub)
                    #Visualise the handle results
                    self.visualise(point2d,point3d_map,normal_map)
                    #Publish spray path poses
                    self.publish_spray_poses(point3d_map,normal_map)
                except Exception as e:
                    rospy.logerr(e)
                    #print(traceback.format_exc())

            #Door
            if self.door_box is not None:
                (point2d,point3d,normal) = self.convert_to_3D(self.door_box)
                try:
                    #Trasform the 3d point into map frame
                    point3d_map = self.rob2map @ np.append(point3d,[[1]],axis=0)
                    normal_map = normal.get_transformed(self.rob2map)
                    self.publish(point3d_map,normal_map,self.door_pose_pub)
                    #Visualise the door results
                    self.visualise(point2d,point3d_map,normal_map)
                except Exception as e:
                    rospy.logerr(e)
                    #print(traceback.format_exc())

            
       
    #TODO: Currently supports returning one feature from the door handle, to the type of Pull doors. In future development care must be taken in determining
    #how many more features will be returned (eg. a lever type of handle would require knowing the point where the force is applied to open the door as well as identifying the point which lever rotates around)
    def convert_to_3D(self, point2D_box:YOLOBoundingBox, is_handle=False, handle_type:Handle=Handle.PULL_DOOR):
        """Given information of a Bounding Box Object enclosing an object and information on camera, lidar, and scan, 
        localise the pixel from the 2d point and output the results of that point in 3D with respect to the robot frame. 
        Outputs the normal to the vertical surface as well as the point originally in 2D.

        
        Args:
            point2D_box (YOLOBoundingBox):  Describes the bounding box of the object passed to it
        """
        (point2d, point3d, normal) = None, None, None

        if point2D_box is not None:
            if (is_handle and handle_type == Handle.PULL_DOOR and self.rgb_image is not None):
                rotation_point, force_location = get_center(image=self.rgb_image,
                                     top_left=[point2D_box.x, point2D_box.y],
                                     width=point2D_box.width,
                                     height=point2D_box.height,
                                     handle_type = handle_type,
                                     simplistic=True)

                point2d = np.array([force_location])
            else:
                point2d = np.array([[point2D_box.x + (point2D_box.width/2)],[point2D_box.y + (point2D_box.height/2)]])   

            (point3d,normal) = localize_pixel(point2d,self.camera,self.lidar,self.scan)
        
        return (point2d,point3d,normal)
    

        




    
# call the class
def main(args):
    rospy.logdebug("Running handle2D_to_3D node...")
    weights = ""
    cfg = ""
    try:
        weights = args[1]
        cfg = args[2]

    except Exception as _:
        weights = DEFAULT_WEIGHTS
        cfg = DEFAULT_CONFIGURATION
        if len(args)>1:
            rospy.logerr("If you want to use other weights and cfg (make sure they're the suitable cfg file for the weights), please add them to the Yolo Folder inside ML and run the node with the following format:")
            rospy.logerr("rosrun dr-phil <filename-in-ML>.weights <filename-in-ML-folder>.cfg")
            rospy.logerr("Default model was trained on two classes: (0) handle ; (1) door - See obj.names")
        
        rospy.loginfo("Using default weights and configuraton...")
    
    rospy.loginfo("weights: " + weights)
    rospy.loginfo("cfg: " + cfg)
    rospy.loginfo("Classes names contained in: " + DEFAULT_OBJ_NAMES)
    camera_parser = Handle3DTransformation(weights,cfg)
    
    rate = rospy.Rate(10)

    try:
        while not rospy.is_shutdown():
                camera_parser.spin()
                rate.sleep()
    except Exception as e:
        print(traceback.format_exc())
    cv2.destroyAllWindows()

# run the code if the node is called
if __name__ == '__main__':
    myargv = rospy.myargv(argv=sys.argv)
    main(myargv)
